{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from skimage import data, io\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZJ000000</th>\n",
       "      <td>../data/train_features/ZJ000000.jpg</td>\n",
       "      <td>S0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ000001</th>\n",
       "      <td>../data/train_features/ZJ000001.jpg</td>\n",
       "      <td>S0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ000002</th>\n",
       "      <td>../data/train_features/ZJ000002.jpg</td>\n",
       "      <td>S0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ000003</th>\n",
       "      <td>../data/train_features/ZJ000003.jpg</td>\n",
       "      <td>S0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ000004</th>\n",
       "      <td>../data/train_features/ZJ000004.jpg</td>\n",
       "      <td>S0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016483</th>\n",
       "      <td>../data/train_features/ZJ016483.jpg</td>\n",
       "      <td>S0093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016484</th>\n",
       "      <td>../data/train_features/ZJ016484.jpg</td>\n",
       "      <td>S0043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016485</th>\n",
       "      <td>../data/train_features/ZJ016485.jpg</td>\n",
       "      <td>S0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016486</th>\n",
       "      <td>../data/train_features/ZJ016486.jpg</td>\n",
       "      <td>S0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016487</th>\n",
       "      <td>../data/train_features/ZJ016487.jpg</td>\n",
       "      <td>S0021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16488 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     filepath   site\n",
       "id                                                  \n",
       "ZJ000000  ../data/train_features/ZJ000000.jpg  S0120\n",
       "ZJ000001  ../data/train_features/ZJ000001.jpg  S0069\n",
       "ZJ000002  ../data/train_features/ZJ000002.jpg  S0009\n",
       "ZJ000003  ../data/train_features/ZJ000003.jpg  S0008\n",
       "ZJ000004  ../data/train_features/ZJ000004.jpg  S0036\n",
       "...                                       ...    ...\n",
       "ZJ016483  ../data/train_features/ZJ016483.jpg  S0093\n",
       "ZJ016484  ../data/train_features/ZJ016484.jpg  S0043\n",
       "ZJ016485  ../data/train_features/ZJ016485.jpg  S0089\n",
       "ZJ016486  ../data/train_features/ZJ016486.jpg  S0095\n",
       "ZJ016487  ../data/train_features/ZJ016487.jpg  S0021\n",
       "\n",
       "[16488 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\"../data/train_features.csv\", index_col=\"id\")\n",
    "train_labels = pd.read_csv(\"../data/train_labels.csv\", index_col=\"id\")\n",
    "\n",
    "train_features[\"filepath\"] = train_features[\"filepath\"].apply(\n",
    "    lambda x: os.path.join(\"/Users/nicholasabad/Desktop/workspace/driven-data-conservision/data\", x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(\n",
    "        self, \n",
    "        training_features: pd.DataFrame,\n",
    "        training_labels: pd.DataFrame,\n",
    "        validation_split: float=0.2,\n",
    "        resized_image_shape: tuple=(224, 224),\n",
    "        random_state: int=42\n",
    "    ):\n",
    "        self.training_features = training_features\n",
    "        self.training_labels = training_labels\n",
    "        self.validation_split = validation_split\n",
    "        self.resized_image_shape = resized_image_shape\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _convert_images_to_numpy_array_and_resize(\n",
    "        self,\n",
    "        paths_to_images: list,\n",
    "    ):\n",
    "        # Helper function to ensure all images are RGB\n",
    "        def ensure_rgb(image):\n",
    "            if len(image.shape) == 2:  # Grayscale (H, W)\n",
    "                return np.stack([image] * 3, axis=-1)  # Convert to (H, W, 3)\n",
    "            elif len(image.shape) == 3 and image.shape[2] == 3:  # Already RGB\n",
    "                return image\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "            \n",
    "        return np.stack([\n",
    "            ensure_rgb(resize(\n",
    "                io.imread(path),\n",
    "                output_shape=self.resized_image_shape,\n",
    "                anti_aliasing=True,\n",
    "            )) / 255.0\n",
    "            for path in tqdm(paths_to_images, desc=\"Loading data into np array:\")\n",
    "        ], axis=0)\n",
    "        \n",
    "    def split_data(self):\n",
    "        # Split the data into a training and validation split.\n",
    "        y = self.training_labels\n",
    "        x = self.training_features.loc[y.index].filepath.to_frame()\n",
    "        \n",
    "        x_train, x_val, y_train, y_val = train_test_split(\n",
    "            x, y, stratify=y, test_size=self.validation_split, random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.x_train = self._convert_images_to_numpy_array_and_resize(x_train[\"filepath\"])\n",
    "        self.x_val = self._convert_images_to_numpy_array_and_resize(x_val[\"filepath\"])\n",
    "        \n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/38kg81v526j7qcgd8gdq8jd00000gn/T/ipykernel_13587/2928171840.py:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for path in tqdm(paths_to_images, desc=\"Loading data into np array:\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f845e4b1e8f74cbbbf6b2dd27de94902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data into np array::   0%|          | 0/13190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(\n",
    "    training_features = train_features,\n",
    "    training_labels = train_labels,\n",
    "    validation_split = 0.2,\n",
    "    resized_image_shape = (224, 224),\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "data_loader.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZJ000000</th>\n",
       "      <td>../data/train_features/ZJ000000.jpg</td>\n",
       "      <td>S0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ000001</th>\n",
       "      <td>../data/train_features/ZJ000001.jpg</td>\n",
       "      <td>S0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ000002</th>\n",
       "      <td>../data/train_features/ZJ000002.jpg</td>\n",
       "      <td>S0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ000003</th>\n",
       "      <td>../data/train_features/ZJ000003.jpg</td>\n",
       "      <td>S0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ000004</th>\n",
       "      <td>../data/train_features/ZJ000004.jpg</td>\n",
       "      <td>S0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016483</th>\n",
       "      <td>../data/train_features/ZJ016483.jpg</td>\n",
       "      <td>S0093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016484</th>\n",
       "      <td>../data/train_features/ZJ016484.jpg</td>\n",
       "      <td>S0043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016485</th>\n",
       "      <td>../data/train_features/ZJ016485.jpg</td>\n",
       "      <td>S0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016486</th>\n",
       "      <td>../data/train_features/ZJ016486.jpg</td>\n",
       "      <td>S0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ016487</th>\n",
       "      <td>../data/train_features/ZJ016487.jpg</td>\n",
       "      <td>S0021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16488 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     filepath   site\n",
       "id                                                  \n",
       "ZJ000000  ../data/train_features/ZJ000000.jpg  S0120\n",
       "ZJ000001  ../data/train_features/ZJ000001.jpg  S0069\n",
       "ZJ000002  ../data/train_features/ZJ000002.jpg  S0009\n",
       "ZJ000003  ../data/train_features/ZJ000003.jpg  S0008\n",
       "ZJ000004  ../data/train_features/ZJ000004.jpg  S0036\n",
       "...                                       ...    ...\n",
       "ZJ016483  ../data/train_features/ZJ016483.jpg  S0093\n",
       "ZJ016484  ../data/train_features/ZJ016484.jpg  S0043\n",
       "ZJ016485  ../data/train_features/ZJ016485.jpg  S0089\n",
       "ZJ016486  ../data/train_features/ZJ016486.jpg  S0095\n",
       "ZJ016487  ../data/train_features/ZJ016487.jpg  S0021\n",
       "\n",
       "[16488 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZJ000625</th>\n",
       "      <td>../data/train_features/ZJ000625.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ008571</th>\n",
       "      <td>../data/train_features/ZJ008571.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ008106</th>\n",
       "      <td>../data/train_features/ZJ008106.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ011175</th>\n",
       "      <td>../data/train_features/ZJ011175.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ003323</th>\n",
       "      <td>../data/train_features/ZJ003323.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ000177</th>\n",
       "      <td>../data/train_features/ZJ000177.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ007834</th>\n",
       "      <td>../data/train_features/ZJ007834.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ015074</th>\n",
       "      <td>../data/train_features/ZJ015074.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ006850</th>\n",
       "      <td>../data/train_features/ZJ006850.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZJ004282</th>\n",
       "      <td>../data/train_features/ZJ004282.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1649 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     filepath\n",
       "id                                           \n",
       "ZJ000625  ../data/train_features/ZJ000625.jpg\n",
       "ZJ008571  ../data/train_features/ZJ008571.jpg\n",
       "ZJ008106  ../data/train_features/ZJ008106.jpg\n",
       "ZJ011175  ../data/train_features/ZJ011175.jpg\n",
       "ZJ003323  ../data/train_features/ZJ003323.jpg\n",
       "...                                       ...\n",
       "ZJ000177  ../data/train_features/ZJ000177.jpg\n",
       "ZJ007834  ../data/train_features/ZJ007834.jpg\n",
       "ZJ015074  ../data/train_features/ZJ015074.jpg\n",
       "ZJ006850  ../data/train_features/ZJ006850.jpg\n",
       "ZJ004282  ../data/train_features/ZJ004282.jpg\n",
       "\n",
       "[1649 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_features.loc[y.index].filepath.to_frame()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_images_to_numpy_array(\n",
    "    paths_to_images: list,\n",
    "):\n",
    "    # Helper function to ensure all images are RGB\n",
    "    def ensure_rgb(image):\n",
    "        if len(image.shape) == 2:  # Grayscale (H, W)\n",
    "            return np.stack([image] * 3, axis=-1)  # Convert to (H, W, 3)\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 3:  # Already RGB\n",
    "            return image\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "        \n",
    "    return np.stack([\n",
    "        ensure_rgb(resize(\n",
    "            io.imread(path),\n",
    "            output_shape=(224, 224),\n",
    "            anti_aliasing=True,\n",
    "        )) / 255.0\n",
    "        for path in tqdm(paths_to_images, desc=\"Loading data into np array...\")\n",
    "    ], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/38kg81v526j7qcgd8gdq8jd00000gn/T/ipykernel_41412/2021609546.py:67: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, row in tqdm(self.y_train.iterrows(), total = self.y_train.shape[0], desc=\"Copying training files...\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6790e87492974b228e53ee1bd303ea69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying training files...:   0%|          | 0/13190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/38kg81v526j7qcgd8gdq8jd00000gn/T/ipykernel_41412/2021609546.py:79: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, row in tqdm(self.y_val.iterrows(), total = self.y_val.shape[0], desc=\"Copying validation files...\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee38adab49034a55b86359645d769dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying validation files...:   0%|          | 0/3298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a train and validate directory with each directory having one of the n classes.\n",
    "\n",
    "class DirectoryCreation():\n",
    "    def __init__(\n",
    "        self, \n",
    "        path_to_directory: str,\n",
    "        training_labels: pd.DataFrame,\n",
    "        training_features: pd.DataFrame,\n",
    "        random_seed: int=42, \n",
    "        validation_split_size: float=0.2\n",
    "    ):\n",
    "        self.path_to_directory = path_to_directory\n",
    "        self.training_labels = training_labels\n",
    "        self.training_features = training_features\n",
    "        self.random_seed = random_seed\n",
    "        self.validation_split_size = validation_split_size\n",
    "        \n",
    "    def create_random_split(self):\n",
    "        # Split the data into a training and validation split.\n",
    "        y = self.training_labels\n",
    "        x = self.training_features.loc[y.index][\"filepath\"].to_frame()\n",
    "        \n",
    "        x_train, x_val, y_train, y_val = train_test_split(\n",
    "            x, y, stratify=y, test_size=self.validation_split_size, random_state=self.random_seed\n",
    "        )\n",
    "        \n",
    "        self.x_train = x_train\n",
    "        self.x_val = x_val\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        \n",
    "    def write_images_to_directory(self):\n",
    "        path_to_training_images = os.path.join(\n",
    "            self.path_to_directory,\n",
    "            \"train\"\n",
    "        )\n",
    "        if os.path.exists(path_to_training_images):\n",
    "            shutil.rmtree(path_to_training_images)\n",
    "        os.mkdir(path_to_training_images)\n",
    "        \n",
    "        path_to_validation_images = os.path.join(\n",
    "            self.path_to_directory,\n",
    "            \"validation\"\n",
    "        )\n",
    "        \n",
    "        if os.path.exists(path_to_validation_images):\n",
    "            shutil.rmtree(path_to_validation_images)\n",
    "        os.mkdir(path_to_validation_images)\n",
    "            \n",
    "            \n",
    "        for column in self.y_train.columns:\n",
    "            train_class_subfolder = os.path.join(\n",
    "                path_to_training_images,\n",
    "                column\n",
    "            )\n",
    "            os.mkdir(train_class_subfolder)\n",
    "            \n",
    "            validation_class_subfolder = os.path.join(\n",
    "                path_to_validation_images,\n",
    "                column\n",
    "            )\n",
    "            os.mkdir(validation_class_subfolder)\n",
    "            \n",
    "        # Determine where to save each file.\n",
    "        list_of_categories = list(self.y_train.columns)\n",
    "        \n",
    "        for idx, row in tqdm(self.y_train.iterrows(), total = self.y_train.shape[0], desc=\"Copying training files...\"):\n",
    "            category = list_of_categories[self.y_train.loc[idx].argmax()]\n",
    "            path_to_original_file = self.x_train.loc[idx, \"filepath\"]\n",
    "            \n",
    "            shutil.copy(\n",
    "                src=path_to_original_file,\n",
    "                dst=os.path.join(\n",
    "                    path_to_training_images,\n",
    "                    category\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        for idx, row in tqdm(self.y_val.iterrows(), total = self.y_val.shape[0], desc=\"Copying validation files...\"):\n",
    "            category = list_of_categories[self.y_val.loc[idx].argmax()]\n",
    "            path_to_original_file = self.x_val.loc[idx, \"filepath\"]\n",
    "            \n",
    "            shutil.copy(\n",
    "                src=path_to_original_file,\n",
    "                dst=os.path.join(\n",
    "                    path_to_validation_images,\n",
    "                    category\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    \n",
    "directory = DirectoryCreation(\n",
    "    path_to_directory = \"/Users/nicholasabad/Desktop/workspace/driven-data-conservision/data\",\n",
    "    training_labels = train_labels,\n",
    "    training_features = train_features,\n",
    "    random_seed = 42,\n",
    "    validation_split_size = 0.2,\n",
    ")\n",
    "\n",
    "directory.create_random_split()\n",
    "directory.write_images_to_directory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13190 images belonging to 8 classes.\n",
      "Found 3298 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Paths to training and validation directories\n",
    "train_dir = \"/Users/nicholasabad/Desktop/workspace/driven-data-conservision/data/train\"\n",
    "val_dir = \"/Users/nicholasabad/Desktop/workspace/driven-data-conservision/data/validation\"\n",
    "\n",
    "# Create ImageDataGenerator for training and validation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,    # Randomly rotate images\n",
    "    width_shift_range=0.2,  # Randomly shift horizontally\n",
    "    height_shift_range=0.2, # Randomly shift vertically\n",
    "    shear_range=0.2,       # Shear transformation\n",
    "    zoom_range=0.2,        # Random zoom\n",
    "    horizontal_flip=True,  # Flip images horizontally\n",
    "    fill_mode=\"nearest\"    # Fill empty pixels after transformations\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0  # Normalize pixel values to [0, 1]\n",
    ")\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory=val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasabad/miniconda3/envs/drivendata/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Match number of classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasabad/miniconda3/envs/drivendata/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m147/412\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 415ms/step - accuracy: 0.2174 - loss: 2.7647"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/drivendata/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drivendata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
